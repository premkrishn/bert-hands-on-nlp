{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOe7eRVpKThQQMmgrmGPV2u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/premkrishn/bert-hands-on-nlp/blob/main/why_similar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUEinO1joTPP"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Function to find common words between two texts\n",
        "def find_common_words(text1, text2):\n",
        "    words1 = set(text1.split())\n",
        "    words2 = set(text2.split())\n",
        "    common_words = words1.intersection(words2)\n",
        "    return common_words\n",
        "\n",
        "# Random HTML document text (you need to load and preprocess it similar to the others)\n",
        "random_html_text = preprocess_text(random_html_content)\n",
        "\n",
        "# Find common words between random HTML document and top similar pages\n",
        "common_words_list = []\n",
        "for page_path in top_similar_pages:\n",
        "    with open(page_path, 'r', encoding='utf-8') as file:\n",
        "        page_content = file.read()\n",
        "        page_text = preprocess_text(page_content)\n",
        "        common_words = find_common_words(random_html_text, page_text)\n",
        "        common_words_list.append((page_path, common_words))\n",
        "\n",
        "# Print common words for each top similar page\n",
        "for page_path, common_words in common_words_list:\n",
        "    print(\"Common words in\", page_path)\n",
        "    print(common_words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word similarity"
      ],
      "metadata": {
        "id": "cFtYPC3HoceH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute similarity with all documents\n",
        "similarities = cosine_similarity(random_html_embedding, embeddings)\n",
        "\n",
        "# Find top similar pages\n",
        "top_indices = similarities.argsort()[-3:][::-1]\n",
        "top_similar_pages = [file_paths[i] for i in top_indices]\n",
        "\n",
        "print(\"Top 3 similar pages:\")\n",
        "for i, page_path in enumerate(top_similar_pages):\n",
        "    print(f\"{i+1}. {page_path}\")\n",
        "\n",
        "# Find sections contributing most to similarity\n",
        "common_sections = []\n",
        "for i, page_path in enumerate(top_similar_pages):\n",
        "    with open(page_path, 'r', encoding='utf-8') as file:\n",
        "        page_content = file.read()\n",
        "        page_text = preprocess_text(page_content)\n",
        "        # Calculate contribution of each section to similarity\n",
        "        section_scores = cosine_similarity(random_html_embedding[i:i+1], embeddings)\n",
        "        # Find the section(s) with highest contribution\n",
        "        max_index = np.argmax(section_scores)\n",
        "        common_sections.append((page_path, max_index))\n",
        "\n",
        "# Print the sections contributing most to similarity for each top similar page\n",
        "for page_path, max_index in common_sections:\n",
        "    print(\"\\nCommon Section in\", page_path)\n",
        "    with open(page_path, 'r', encoding='utf-8') as file:\n",
        "        page_content = file.read()\n",
        "        page_text = preprocess_text(page_content)\n",
        "        # Split page into sections (adjust this as per your HTML structure)\n",
        "        sections = page_text.split('\\n\\n')\n",
        "        print(sections[max_index])\n"
      ],
      "metadata": {
        "id": "aInGbD4Yoee3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "context similarity"
      ],
      "metadata": {
        "id": "GIlQ4VUborK3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6cXe11ohooT-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}