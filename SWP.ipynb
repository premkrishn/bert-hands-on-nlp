{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNGocLOMVCPb50zBpP5b9y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/premkrishn/bert-hands-on-nlp/blob/main/SWP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import os\n",
        "import re\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "def download_website(url, folder):\n",
        "    # Create folder if it doesn't exist\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "\n",
        "    # Set to keep track of visited URLs\n",
        "    visited_urls = set()\n",
        "\n",
        "    # Download the main page and follow internal links recursively\n",
        "    download_page(url, folder, visited_urls)\n",
        "\n",
        "def download_page(url, folder, visited_urls):\n",
        "    try:\n",
        "        # Check if the URL has already been visited\n",
        "        if url in visited_urls:\n",
        "            return\n",
        "        visited_urls.add(url)\n",
        "\n",
        "        # Get the webpage content\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            # Save page\n",
        "            save_page(response.content, folder, get_filename(url))\n",
        "\n",
        "            # Find all links on the page\n",
        "            links = soup.find_all('a', href=True)\n",
        "            for link in links:\n",
        "                href = urljoin(url, link['href'])\n",
        "                # Check if the link is internal\n",
        "                if href.startswith(url):\n",
        "                    # Download the linked page\n",
        "                    download_page(href, folder, visited_urls)\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading {url}: {str(e)}\")\n",
        "\n",
        "def save_page(content, folder, filename):\n",
        "    with open(os.path.join(folder, filename), 'wb') as f:\n",
        "        f.write(content)\n",
        "\n",
        "def get_filename(url):\n",
        "    # Extract filename from URL\n",
        "    filename = re.sub(r'^https?://', '', url)\n",
        "    filename = re.sub(r'[^a-zA-Z0-9\\-._]', '_', filename)\n",
        "    if filename.endswith('/'):\n",
        "        filename += \"index\"\n",
        "    filename += \".html\"\n",
        "    return filename\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # URL of the website\n",
        "    url = \"https://am.jpmorgan.com/us/en/asset-management/liq/\"\n",
        "    # Folder to save web pages\n",
        "    folder = \"jpmorgan_website\"\n",
        "\n",
        "    download_website(url, folder)\n",
        "    print(\"Website downloaded successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCFPS7me-wCR",
        "outputId": "1ba6a2d2-42c1-47c4-9e51-a0b3469360d3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Website downloaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import os\n",
        "import re\n",
        "from urllib.parse import urlparse, unquote\n",
        "\n",
        "def download_website(url, folder):\n",
        "    # Create folder if it doesn't exist\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "\n",
        "    # Set to keep track of visited URLs\n",
        "    visited_urls = set()\n",
        "\n",
        "    # Download the main page and follow internal links recursively\n",
        "    download_page(url, folder, visited_urls)\n",
        "\n",
        "def download_page(url, folder, visited_urls):\n",
        "    try:\n",
        "        # Check if the URL has already been visited\n",
        "        if url in visited_urls:\n",
        "            return\n",
        "        visited_urls.add(url)\n",
        "\n",
        "        # Get the webpage content\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            # Save page\n",
        "            filename = get_filename(url)\n",
        "            save_page(response.content, folder, filename)\n",
        "\n",
        "            # Find all links on the page\n",
        "            links = soup.find_all('a', href=True)\n",
        "            for link in links:\n",
        "                href = link['href']\n",
        "                # Check if the link is internal\n",
        "                if is_internal_link(url, href):\n",
        "                    # Convert relative URLs to absolute URLs\n",
        "                    href = urljoin(url, href)\n",
        "                    # Download the linked page\n",
        "                    download_page(href, folder, visited_urls)\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading {url}: {str(e)}\")\n",
        "\n",
        "def save_page(content, folder, filename):\n",
        "    with open(os.path.join(folder, filename), 'wb') as f:\n",
        "        f.write(content)\n",
        "\n",
        "def get_filename(url):\n",
        "    # Parse the URL to extract the filename\n",
        "    parsed_url = urlparse(url)\n",
        "    # Unquote special characters in the path\n",
        "    filename = unquote(parsed_url.path)\n",
        "    # Remove leading and trailing slashes\n",
        "    filename = filename.strip('/')\n",
        "    # Replace slashes and special characters with underscores\n",
        "    filename = re.sub(r'[/\\\\?%*:|\"<>]', '_', filename)\n",
        "    # If the filename is empty, use \"index.html\"\n",
        "    if not filename:\n",
        "        filename = \"index.html\"\n",
        "    return filename\n",
        "\n",
        "def is_internal_link(base_url, link):\n",
        "    # Check if the link is internal (starts with the base URL)\n",
        "    return link.startswith(base_url)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # URL of the website\n",
        "    url = \"https://am.jpmorgan.com/us/en/asset-management/liq/\"\n",
        "    # Folder to save web pages\n",
        "    folder = \"jpmorgan_website1\"\n",
        "\n",
        "    download_website(url, folder)\n",
        "    print(\"Website downloaded successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvgQdMqgNbVl",
        "outputId": "a1c88c21-f04c-491a-bbea-c4cb92aefb03"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Website downloaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import re\n",
        "from urllib.parse import urlparse, unquote\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "def load_pages(folder):\n",
        "    pages = []\n",
        "    for filename in os.listdir(folder):\n",
        "        if filename.endswith(\".html\"):\n",
        "            with open(os.path.join(folder, filename), 'r', encoding=\"utf-8\") as f:\n",
        "                content = f.read()\n",
        "                pages.append(content)\n",
        "    return pages\n",
        "\n",
        "def preprocess_page(page):\n",
        "    # Remove HTML tags and unnecessary spaces\n",
        "    soup = BeautifulSoup(page, 'html.parser')\n",
        "    text = re.sub(r'\\s+', ' ', soup.get_text(strip=True))\n",
        "    return text\n",
        "\n",
        "def encode_pages(pages):\n",
        "    model = SentenceTransformer(\"distilbert-base-nli-mean-tokens\")\n",
        "    embeddings = model.encode(pages)\n",
        "    return embeddings\n",
        "\n",
        "def calculate_similarity(embeddings):\n",
        "    similarities = cosine_similarity(embeddings)\n",
        "    np.fill_diagonal(similarities, 0)  # Set diagonal elements to 0 to avoid self-similarity\n",
        "    return similarities\n",
        "\n",
        "def find_top_similar_pages(similarities, page_urls, top_k=5):\n",
        "    top_similar_pages = {}\n",
        "    for i, url in enumerate(page_urls):\n",
        "        similar_pages_indices = np.argsort(similarities[i])[::-1][:top_k]\n",
        "        similar_pages = [page_urls[idx] for idx in similar_pages_indices]\n",
        "        top_similar_pages[url] = similar_pages\n",
        "    return top_similar_pages\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Folder containing downloaded web pages\n",
        "    folder = \"jpmorgan_website\"\n",
        "\n",
        "    # Load and preprocess pages\n",
        "    pages = load_pages(folder)\n",
        "    preprocessed_pages = [preprocess_page(page) for page in pages]\n",
        "\n",
        "    # Encode pages\n",
        "    embeddings = encode_pages(preprocessed_pages)\n",
        "\n",
        "    # Calculate similarity between pages\n",
        "    similarities = calculate_similarity(embeddings)\n",
        "\n",
        "    # Get page URLs\n",
        "    page_urls = [get_filename(url) for url in os.listdir(folder) if url.endswith(\".html\")]\n",
        "\n",
        "    # Find top similar pages for each page\n",
        "    top_similar_pages = find_top_similar_pages(similarities, page_urls)\n",
        "    for url, similar_pages in top_similar_pages.items():\n",
        "        print(f\"Top 5 similar pages for: {url}\")\n",
        "        for i, page in enumerate(similar_pages, start=1):\n",
        "            print(f\"{i}. {page}\")\n",
        "        print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkHfBEpbgmQK",
        "outputId": "6d8635b1-6f56-4bad-9941-18ff7a1f9695"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Website downloaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "def load_pages(folder):\n",
        "    pages = []\n",
        "    for filename in os.listdir(folder):\n",
        "        if filename.endswith(\".html\"):\n",
        "            with open(os.path.join(folder, filename), 'r', encoding=\"utf-8\") as f:\n",
        "                content = f.read()\n",
        "                pages.append(content)\n",
        "    return pages\n",
        "\n",
        "def preprocess_page(page):\n",
        "    # Remove HTML tags and unnecessary spaces\n",
        "    soup = BeautifulSoup(page, 'html.parser')\n",
        "    text = re.sub(r'\\s+', ' ', soup.get_text(strip=True))\n",
        "    return text\n",
        "\n",
        "def calculate_similarity(pages):\n",
        "    # TF-IDF vectorization\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(pages)\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    similarities = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "    np.fill_diagonal(similarities, 0)  # Set diagonal elements to 0 to avoid self-similarity\n",
        "    return similarities\n",
        "\n",
        "def find_top_similar_pages(similarities, page_urls, top_k=5):\n",
        "    top_similar_pages = {}\n",
        "    for i, url in enumerate(page_urls):\n",
        "        similar_pages_indices = np.argsort(similarities[i])[::-1][:top_k]\n",
        "        similar_pages = [page_urls[idx] for idx in similar_pages_indices]\n",
        "        top_similar_pages[url] = similar_pages\n",
        "    return top_similar_pages\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Folder containing downloaded web pages\n",
        "    folder = \"jpmorgan_website\"\n",
        "\n",
        "    # Load and preprocess pages\n",
        "    pages = load_pages(folder)\n",
        "    preprocessed_pages = [preprocess_page(page) for page in pages]\n",
        "\n",
        "    # Calculate similarity between pages\n",
        "    similarities = calculate_similarity(preprocessed_pages)\n",
        "\n",
        "    # Get page URLs\n",
        "    page_urls = [get_filename(url) for url in os.listdir(folder) if url.endswith(\".html\")]\n",
        "\n",
        "    # Find top similar pages for each page\n",
        "    top_similar_pages = find_top_similar_pages(similarities, page_urls)\n",
        "    for url, similar_pages in top_similar_pages.items():\n",
        "        print(f\"Top 5 similar pages for: {url}\")\n",
        "        for i, page in enumerate(similar_pages, start=1):\n",
        "            print(f\"{i}. {page}\")\n",
        "        print()\n"
      ],
      "metadata": {
        "id": "RluPVCC3hKqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yy0T_k0eh-8L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}