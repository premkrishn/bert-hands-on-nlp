{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/premkrishn/bert-hands-on-nlp/blob/main/Similarity_on_downloaded.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yVXmREOSbFd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load HTML files\n",
        "file_paths = glob.glob('SWP/jpmorgan_website_FA/*.html')\n",
        "\n",
        "# Initialize BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    # Remove HTML tags\n",
        "    soup = BeautifulSoup(text, 'html.parser')\n",
        "\n",
        "    # Remove footer elements\n",
        "    footer_elements = soup.find_all(\"footer\")\n",
        "    for footer_element in footer_elements:\n",
        "        footer_element.extract()\n",
        "\n",
        "    text = soup.get_text()\n",
        "\n",
        "    # Remove extra spaces\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "# Function to get BERT embeddings\n",
        "def get_bert_embeddings(text):\n",
        "    inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    embeddings = torch.mean(outputs.last_hidden_state, dim=1).squeeze().numpy()\n",
        "    return embeddings\n",
        "\n",
        "# Load, preprocess HTML files, and compute BERT embeddings\n",
        "embeddings = []\n",
        "for file_path in file_paths:\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        html_content = file.read()\n",
        "        text = preprocess_text(html_content)\n",
        "        embeddings.append(get_bert_embeddings(text))\n",
        "\n",
        "# Convert list of embeddings to numpy array\n",
        "embeddings = np.array(embeddings)\n",
        "\n",
        "# Function to compute similarity\n",
        "def compute_similarity(query_embedding, document_embeddings):\n",
        "    return cosine_similarity([query_embedding], document_embeddings)[0]\n",
        "\n",
        "# Random HTML document (you need to load and preprocess it similar to the others)\n",
        "random_html_text = preprocess_text(random_html_content)\n",
        "random_html_embedding = get_bert_embeddings(random_html_text)\n",
        "\n",
        "# Compute similarity with all documents\n",
        "similarities = compute_similarity(random_html_embedding, embeddings)\n",
        "\n",
        "# Find top similar pages\n",
        "top_indices = similarities.argsort()[-3:][::-1]\n",
        "top_similar_pages = [file_paths[i] for i in top_indices]\n",
        "\n",
        "print(\"Top 3 similar pages:\")\n",
        "for page in top_similar_pages:\n",
        "    print(page)\n"
      ]
    }
  ]
}